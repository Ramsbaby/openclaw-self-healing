# 품질 체크 V4.0

> 점수 시스템 폐지. 객관 지표 + 목표 대비 측정 + 의사결정 투명성.

## 출력 형식

```
╭─────── 🔍 ───────╮
│ **품질 체크 V4.0**
├───────────────────
│ **객관 지표**
│ 　도구: X회 호출 / Y회 실패 (실패율: Z%)
│ 　응답: X초 (목표: <15초) [✓/✗]
│ 　재시도: X회 (목표: 0회) [✓/✗]
│ 　정확도: N개 중 M개 유효 (M/N%)
│ 　토큰: X tokens (예산 대비: Y%)
│
│ **의사결정 추론** (CoT)
│ 　• 도구 선택: [왜 이 도구를 사용했는가]
│ 　• 접근 방법: [왜 이 방법을 택했는가]
│ 　• 트레이드오프: [어떤 선택지를 고려했는가]
│
│ **이번 실패/미흡** (필수 1개+)
│ 　• [구체적 사항]
│
│ **즉시 개선**
│ 　• [다음부터 적용할 것]
╰─────── 🔍 ───────╯
```

## 규칙

### 1. 점수 금지
- ❌ "9/10", "10점", "총점" 등 점수 표현 금지
- ❌ "잘했다", "완벽", "문제없음" 등 자화자찬 금지

### 2. 목표 대비 측정

**필수 메트릭**:
- 도구 호출 횟수 / 실패 횟수 / **실패율 계산**
- 응답 생성 시간 / **목표 시간 대비**
- 재시도 횟수 / **목표(0회) 대비**
- 정확도 (API 응답/데이터 유효성)
- 토큰 사용량 / **예산 대비 비율**

**목표 기준**:
- 응답 시간: <15초 (복잡한 작업 <30초)
- 재시도: 0회 (API 에러는 예외)
- 도구 실패율: <5%
- 정확도: >95%
- 토큰: 예산의 <80%

**표시 방법**:
- 목표 달성: [✓]
- 목표 미달성: [✗]

### 3. 의사결정 추론 (CoT) 필수

**왜 기록하는가?**
- 의사결정 과정의 투명성 확보
- 같은 상황에서 더 나은 선택 가능
- 트레이드오프 이해를 통한 개선

**기록 항목**:
1. **도구 선택**: 왜 A 도구를 사용했는가? (B, C는 왜 안 썼는가?)
2. **접근 방법**: 왜 이 방법을 택했는가? (다른 방법은?)
3. **트레이드오프**: 어떤 선택지를 고려했는가? (속도 vs 정확도, 간결 vs 완전성 등)

**예시**:
```
│ **의사결정 추론**
│ 　• 도구 선택: Grep 대신 Task/Explore 사용 → 광범위 검색 필요
│ 　• 접근 방법: 병렬 검색 대신 순차 → Rate Limit 회피
│ 　• 트레이드오프: 속도 희생, 정확도 우선 (사용자가 결과 품질 중시)
```

### 4. 실패/미흡 필수

**최소 1개 이상** 작성 필수. "없음" 금지.

찾는 방법:
- 더 간결할 수 있었나?
- 불필요한 도구 호출 있었나?
- 사용자가 재질문해야 했나?
- 포맷이 최적이었나?
- 정보 누락은?
- **목표 미달성 항목은?**
- **CoT에서 더 나은 선택이 있었나?**

아무리 잘해도 **개선 여지는 항상 있다.**

### 5. 즉시 개선

다음 응답부터 바로 적용할 구체적 액션.
추상적 다짐 금지 ("더 노력하겠다" ❌)

**좋은 예**:
- ✓ "다음엔 Grep 전에 파일 개수 확인 (ls | wc -l)"
- ✓ "병렬 검색 시 2-3개씩 묶어서 실행"
- ✓ "응답 시간 15초 초과 시 중간 진행 상황 출력"

**나쁜 예**:
- ✗ "더 빠르게 하겠다"
- ✗ "주의하겠다"
- ✗ "노력하겠다"

## 저장

```bash
cat >> ~/openclaw/memory/self-review-$(date '+%Y-%m-%d').md << 'EOF'
## HH:MM 크론명
[품질 체크 박스]
EOF
```

## 메트릭 자동 수집 (선택)

일부 메트릭은 자동 수집 가능:

```javascript
// 예시: cron에서 자동 측정
const startTime = Date.now();
const toolCalls = [];
const errors = [];

try {
  // 작업 실행
  const result = await executeCron();

  // 자동 메트릭
  const metrics = {
    duration: (Date.now() - startTime) / 1000,
    toolCalls: toolCalls.length,
    failures: errors.length,
    failureRate: (errors.length / toolCalls.length * 100).toFixed(1)
  };

  // self-review에 자동 포함
} catch (e) {
  errors.push(e);
}
```

## 주간 외부 검증

매주 일요일 23:30 Opus가 검토:
- 실패/미흡이 실제로 개선됐는지
- 같은 실수 반복 여부
- 개선 패턴 분석
- **목표 달성률 트렌드**
- **CoT 품질 평가**

## 월간 KPI 리포트

매월 첫째 주 월요일 자동 생성:

```markdown
# 2026-02 자가개선 KPI

## 목표 달성률
- 응답 시간 목표: 85% 달성 (17/20 cron)
- 재시도 0회: 90% 달성 (18/20 cron)
- 도구 실패율 <5%: 95% 달성 (19/20 cron)

## 트렌드
- 평균 응답 시간: 12s → 10s (⬇️ 17%)
- 도구 실패율: 6% → 3% (⬇️ 50%)
- 같은 실수 반복: 3회 → 0회 (✅)

## 개선 필요 영역
- [자동 식별된 약점]
```

## V3.3 대비 변경사항

**추가됨**:
- 목표 대비 측정 ([✓/✗] 표시)
- 실패율 계산
- 정확도 측정
- 토큰 사용량 추적
- **의사결정 추론 (CoT) 섹션**
- 월간 KPI 리포트

**유지됨**:
- 점수 시스템 폐지
- 실패/미흡 필수 (최소 1개)
- 즉시 개선 (구체적 액션)
- 주간 외부 검증

**목적**:
- V3.3: 자화자찬 방지, 객관성 확보
- V4.0: + 정량적 개선 추적 + 의사결정 투명성

## 마이그레이션 가이드

### 기존 cron 업데이트

1. `~/openclaw/scripts/add-self-review.js` 수정
2. Template path: `v3.3.md` → `v4.0.md`
3. 각 cron에 목표 설정 추가

### 점진적 전환

**Week 1**: 5개 cron을 V4.0으로 시범 적용
**Week 2**: 결과 검토 후 전체 확대
**Week 3**: V3.3 완전 폐기

### A/B Testing

실험군과 대조군으로 나눠서 비교:
- 실험군 (10개 cron): V4.0 적용
- 대조군 (10개 cron): V3.3 유지
- 4주 후 비교: 어느 쪽이 더 많은 개선?

## 참고

- Galileo AI - Self-Evaluation Framework
- Microsoft Agent Lightning
- OpenAI Self-Evolving Agents
- Workday Performance-Driven Agent KPIs
